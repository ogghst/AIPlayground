# chatbot_integration.py

import logging
import asyncio
from typing import AsyncGenerator, Dict, Any

# Import necessary components
from state_management import StateManager, ConversationState, UserInfo
from graph_definition import create_chatbot_graph # Import the function that creates the compiled graph app
from input_processing import process_user_input # Import input processing

# Configure basic logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class ChatbotBridge:
    """
    Bridges incoming requests (e.g., from WebSockets) to the LangGraph application.
    Manages conversation state and invokes the graph execution.
    """
    def __init__(self):
        # Initialize dependencies
        self.state_manager = StateManager()
        try:
            # Create/load the compiled LangGraph application
            self.graph_app = create_chatbot_graph()
            logging.info("ChatbotBridge initialized with compiled LangGraph app.")
        except Exception as e:
            logging.error(f"Failed to create or compile LangGraph app: {e}", exc_info=True)
            self.graph_app = None # Ensure graph is None if init fails

    async def _get_or_create_state(self, conversation_id: str, user_info_details: Dict[str, Any] = None) -> ConversationState:
        """Loads existing state or creates a new one."""
        state = self.state_manager.get_state(conversation_id)
        if not state:
            logging.info(f"No existing state found for conversation {conversation_id}. Creating new state.")
            # Create UserInfo if details are provided
            user_info = UserInfo(**user_info_details) if user_info_details else None
            state = self.state_manager.create_new_state(conversation_id, user_info)
        else:
            logging.debug(f"Loaded existing state for conversation {conversation_id}")
        return state

    async def process_message_stream(self, conversation_id: str, user_message: str, user_identifier: Any) -> AsyncGenerator[str, None]:
        """
        Processes a user message through the LangGraph app and streams responses.

        Args:
            conversation_id: The unique ID for the conversation.
            user_message: The message content from the user.
            user_identifier: An identifier for the user (e.g., SID, user ID).

        Yields:
            Partial responses (strings) as they are generated by the graph/LLM.
        """
        logging.info(f"Processing message for conversation {conversation_id} from user {user_identifier}")

        if not self.graph_app:
            logging.error("Graph application is not initialized. Cannot process message.")
            yield "Error: Chatbot core is not available."
            return

        try:
            # 1. Get or create conversation state
            # TODO: Pass actual user details if available from authenticated session
            state = await self._get_or_create_state(conversation_id)

            # 2. Process and add user input to state
            processed_input = process_user_input(user_message)
            # Use user_identifier (e.g., user_id if available, otherwise sid) as sender?
            sender_id = str(user_identifier) # Ensure it's a string
            state.add_user_message(processed_input['normalized_text'], metadata={'original': processed_input['original_text'], 'sender_id': sender_id})

            # 3. Prepare config for LangGraph run (for checkpointing)
            config = {"configurable": {"thread_id": conversation_id}}

            # 4. Invoke the graph using astream to get events
            full_response = ""
            async for event in self.graph_app.astream(state.to_dict(), config=config):
                # --- Stream Processing Logic ---
                # Decide what to yield based on graph events.
                # This example yields content from 'prepare_response' node assuming it holds the final message.
                # A more sophisticated approach might involve yielding intermediate LLM tokens directly
                # if the LLM provider/LangGraph setup supports streaming tokens within nodes.

                event_type = list(event.keys())[0]
                event_data = event[event_type]
                logging.debug(f"Graph Event ({conversation_id}): {event_type}") # Log event type

                # Example: Yield content when the 'prepare_response' node finishes
                # Assumes this node adds the final assistant message to history
                if event_type == "prepare_response":
                    # The state within event_data should be the updated state *after* the node ran
                    if isinstance(event_data, dict) and "history" in event_data:
                         # Re-create state object from dict to use methods (if needed, or just access dict)
                         current_event_state = ConversationState.from_dict(event_data)
                         if current_event_state.history.messages:
                             last_msg = current_event_state.history.messages[-1]
                             if last_msg.sender == 'assistant':
                                 response_content = last_msg.content
                                 # In a true token stream, you'd yield deltas.
                                 # Here, we yield the whole message at once when the node finishes.
                                 yield response_content
                                 full_response = response_content # Keep track of the final message

                # Example: Handle errors signaled by the graph
                if event_type == "handle_error":
                     error_message = "An error occurred during processing."
                     if isinstance(event_data, dict) and "error_info" in event_data and event_data["error_info"]:
                         error_message = f"Error: {event_data['error_info'].get('error', 'Unknown error')}"
                     yield error_message # Yield the error message to the client
                     logging.error(f"Error processed in graph for {conversation_id}: {event_data.get('error_info')}")
                     # Stop further processing for this turn after error
                     break

                # If we reach the end, log it
                if event_type == END:
                     logging.debug(f"Graph execution reached END for conversation {conversation_id}")
                     # The state manager should have saved the final state via checkpointer


            # Fallback if no response yielded (e.g., graph ended unexpectedly)
            if not full_response:
                 logging.warning(f"Graph execution completed for {conversation_id} but no response was yielded/captured.")
                 # yield "..." # Optionally yield a default message

        except Exception as e:
            logging.error(f"Error during message processing stream for conversation {conversation_id}: {e}", exc_info=True)
            yield f"Error processing your request: {e}" # Yield error back to client 